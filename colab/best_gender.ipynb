{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Person_Attributes_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mashruravi/eip-hvc/blob/master/colab/best_gender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39CDYn8davye",
        "colab_type": "code",
        "outputId": "6b20a71c-be92-466b-f1e8-dfeb15a45f3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "!cp /content/gdrive/My\\ Drive/hvc_encoded_3.csv /content/\n",
        "# `hvc_encoded.csv` and `resized` should be available\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/              hvc_encoded_3.csv  \u001b[01;34mresized\u001b[0m/\n",
            "hvc_annotations.csv  model.h5           \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9gmFOPOf8x7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLgMA5QfgaMq",
        "colab_type": "code",
        "outputId": "e8f117ae-a4a0-4031-f7b4-6bf2cd9c70f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df = pd.read_csv('hvc_encoded_3.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>qual_Average</th>\n",
              "      <th>qual_Bad</th>\n",
              "      <th>qual_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>bag_Daily/Office/Work Bag</th>\n",
              "      <th>bag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>bag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resized/5.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0  resized/1.jpg              0  ...                        1              0\n",
              "1  resized/2.jpg              1  ...                        1              0\n",
              "2  resized/3.jpg              0  ...                        1              0\n",
              "3  resized/4.jpg              0  ...                        1              0\n",
              "4  resized/5.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqpBIrV_hiOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWxisPJ-mZ9i",
        "colab_type": "code",
        "outputId": "8b41b8ae-4eae-4c1b-ea2f-557dea4bbf19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhA6Co__X9xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Columns for one-hot encoded labels\n",
        "def get_columns_by_prefix(prefix):\n",
        "  return [col for col in df if col.startswith(prefix)]\n",
        "\n",
        "_gender_columns = get_columns_by_prefix('gender')\n",
        "_qual_columns_ = get_columns_by_prefix('qual')\n",
        "_age_columns_ = get_columns_by_prefix('age')\n",
        "_weight_columns_ = get_columns_by_prefix('weight')\n",
        "_bag_columns_ = get_columns_by_prefix('bag')\n",
        "_footwear_columns_ = get_columns_by_prefix('footwear')\n",
        "_emotion_columns_ = get_columns_by_prefix('emotion')\n",
        "_bodypose_columns_ = get_columns_by_prefix('bodypose')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou65r4l1gGmK",
        "colab_type": "code",
        "outputId": "0a9ce044-e406-4ec9-dbb9-cd43e3715ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9WupFC_pIjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import cv2\n",
        "from __future__ import division\n",
        "\n",
        "class DataGenerator(tensorflow.keras.utils.Sequence):\n",
        "  \n",
        "  def __init__(self, df, batch_size=32, shuffle=True):\n",
        "    self.df = df\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle = shuffle\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\" return X=224,224,3,batch_size and y=num_classes,batch_size\"\"\"\n",
        "    batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "    items = self.df.iloc[batch_slice]\n",
        "    images = np.stack([cv2.imread(item['image_path']) for _, item in items.iterrows()])\n",
        "    images = images/255\n",
        "    target = {\n",
        "        'gender_output': items[_gender_columns].values,\n",
        "        'imagequality_output': items[_qual_columns_].values,\n",
        "        'age_output': items[_age_columns_].values,\n",
        "        'weight_output': items[_weight_columns_].values,\n",
        "        'bag_output': items[_bag_columns_].values,\n",
        "        'footwear_output': items[_footwear_columns_].values,\n",
        "        'emotion_output': items[_emotion_columns_].values,\n",
        "        'bodypose_output': items[_bodypose_columns_].values\n",
        "    }\n",
        "    return images, target\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    \"\"\"Shuffle data after each epoch\"\"\"\n",
        "    if self.shuffle == True:\n",
        "      self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNzE64-8ZZue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=16\n",
        "train_gen = DataGenerator(train_df, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_gen = DataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vASj_zzMZsIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, targets = next(iter(train_gen))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiFbKBHaavlk",
        "colab_type": "code",
        "outputId": "1d5ccd01-4e06-4a28-e4a2-60e49fc08788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def get_unit_count(value_output):\n",
        "  if (len(value_output.shape) > 1):\n",
        "    return value_output.shape[1]\n",
        "  else:\n",
        "    return 1\n",
        "num_units = { k.split(\"_output\")[0]: get_unit_count(v) for k, v in targets.items() }\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'bodypose': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'imagequality': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNe-tVTGXzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import six\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    Dropout\n",
        ")\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D,\n",
        "    GlobalAveragePooling2D\n",
        ")\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "weight_decay = 0.005\n",
        "\n",
        "def initial_conv(input, i):\n",
        "    x = Conv2D(100, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=f\"conv_init_{i}\")(input)\n",
        "\n",
        "    channel_axis = -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=f\"BN_init_{i}\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(idx, init, base, k, strides=(1, 1)):\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv_expand_\"+str(idx)+\"_1\")(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN_expand\"+str(idx))(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv_expand_\"+str(idx)+\"_2\")(x)\n",
        "\n",
        "    skip = Conv2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv_expand_\"+str(idx)+\"_3\")(init)\n",
        "\n",
        "    m = add([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(idx, input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN1_\"+str(idx)+\"_1\")(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv1_\"+str(idx)+\"_1\")(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(rate=dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN1_\"+str(idx)+\"_2\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv1_\"+str(idx)+\"_2\")(x)\n",
        "\n",
        "    m = add([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(idx, input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN2_\"+str(idx)+\"_1\")(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv2_\"+str(idx)+\"_1\")(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(rate=dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN2_\"+str(idx)+\"_2\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False,name=\"conv2_\"+str(idx)+\"_2\")(x)\n",
        "\n",
        "    m = add([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(idx, input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN3_\"+str(idx)+\"_1\")(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv3_\"+str(idx)+\"_1\")(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(rate=dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=\"BN3_\"+str(idx)+\"_2\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False, name=\"conv3_\"+str(idx)+\"_2\")(x)\n",
        "\n",
        "    m = add([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_layer, N=2, k=1, dropout=0.0, verbose=1, index=0):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = input_layer\n",
        "\n",
        "    x = initial_conv(ip, index)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(f\"{index}_1\", x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(f\"{index}_i\", x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=f\"BN_{index}_1\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(f\"{index}_2\", x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(f\"{index}_i\", x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=f\"BN_{index}_2\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(f\"{index}_3\", x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(f\"{index}_i\", x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform', name=f\"BN_{index}_3\")(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "    # x = Conv2D(nb_classes, (1,1), padding=\"same\", name=\"conv_final\")(x)\n",
        "    # x = GlobalAveragePooling2D(name=\"GAP\")(x)\n",
        "    # x = Activation(\"softmax\")(x)\n",
        "\n",
        "    # model = Model(ip, x)\n",
        "\n",
        "    # if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    # return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ-vVn9NbNue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_decay=0.005\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    Dropout\n",
        ")\n",
        "from keras.layers import (\n",
        "    Convolution2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D,\n",
        "    GlobalAveragePooling2D\n",
        ")\n",
        "from keras.layers import add\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "def conv_bn_relu(backbone, filters):\n",
        "  backbone.add(Convolution2D(filters, (3, 3), dilation_rate=(2, 2)))\n",
        "  backbone.add(BatchNormalization())\n",
        "  backbone.add(Activation('relu'))\n",
        "  backbone.add(Dropout(0.2))\n",
        "\n",
        "def get_backbone(backbone):\n",
        "  backbone.add(Convolution2D(16, (3, 3),dilation_rate=(2, 2),input_shape=(224, 224, 3)))\n",
        "  backbone.add(BatchNormalization())\n",
        "  backbone.add(Activation('relu'))\n",
        "  backbone.add(Dropout(0.2))\n",
        "  conv_bn_relu(backbone, 16)\n",
        "\n",
        "  conv_bn_relu(backbone, 32)\n",
        "  conv_bn_relu(backbone, 32)\n",
        "\n",
        "  conv_bn_relu(backbone, 48)\n",
        "  conv_bn_relu(backbone, 48)\n",
        "\n",
        "  conv_bn_relu(backbone, 64)\n",
        "  conv_bn_relu(backbone, 64)\n",
        "\n",
        "  conv_bn_relu(backbone, 96)\n",
        "  conv_bn_relu(backbone, 96)\n",
        "\n",
        "  conv_bn_relu(backbone, 128)\n",
        "  backbone.add(GlobalAveragePooling2D())\n",
        "\n",
        "def get_neck(input_layer, index):\n",
        "  return input_layer\n",
        "  # x = GlobalAveragePooling2D()(x)\n",
        "  # x = Flatten()(x)\n",
        "  # x = Dropout(rate=0.2)(x)\n",
        "  # x = Dense(32, activation='sigmoid')(x)\n",
        "  # x = Dropout(rate=0.2)(x)\n",
        "  # x = Dense(64, activation='sigmoid')(x)\n",
        "  # return x\n",
        "\n",
        "def get_head(name, input_layer):\n",
        "  return Dense(num_units[name], activation='softmax', name=f'{name}_output')(input_layer)\n",
        "\n",
        "def make_model():\n",
        "\n",
        "  backbone = Sequential()\n",
        "\n",
        "  get_backbone(backbone)\n",
        "\n",
        "\n",
        "  # heads\n",
        "  age = get_head('age', get_neck(backbone.output, 1))\n",
        "  bag = get_head('bag', get_neck(backbone.output, 2))\n",
        "  bodypose = get_head('bodypose', get_neck(backbone.output, 3))\n",
        "  emotion = get_head('emotion', get_neck(backbone.output, 4))\n",
        "  footwear = get_head('footwear', get_neck(backbone.output, 5))\n",
        "  gender = get_head('gender', get_neck(backbone.output, 6))\n",
        "  imagequality = get_head('imagequality', get_neck(backbone.output, 7))\n",
        "  weight = get_head('weight', get_neck(backbone.output, 8))\n",
        "\n",
        "  model = Model(\n",
        "      inputs = backbone.input,\n",
        "      outputs = age\n",
        "      # outputs = [age, bag, bodypose, emotion, footwear, gender, imagequality, weight]\n",
        "  )\n",
        "  \n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "277UKn9zfKDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_compiled_model():\n",
        "\n",
        "  model = make_model()\n",
        "\n",
        "  model.compile(\n",
        "      optimizer='adam',\n",
        "      loss='categorical_crossentropy',\n",
        "      # loss={\n",
        "      #     # 'gender_output': 'categorical_crossentropy',\n",
        "      #     # 'imagequality_output': 'categorical_crossentropy',\n",
        "      #     'age_output': 'categorical_crossentropy',\n",
        "      #     # 'weight_output': 'categorical_crossentropy',\n",
        "      #     # 'bag_output': 'categorical_crossentropy',\n",
        "      #     # 'footwear_output': 'categorical_crossentropy',\n",
        "      #     # 'emotion_output': 'categorical_crossentropy',\n",
        "      #     # 'bodypose_output': 'categorical_crossentropy'\n",
        "      # },\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Cs_3zwHYyy",
        "colab_type": "code",
        "outputId": "7be51fdf-13d3-4ec0-d5ab-88d271857254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = get_compiled_model()\n",
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_167_input (InputLayer (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_167 (Conv2D)          (None, 220, 220, 16)      448       \n",
            "_________________________________________________________________\n",
            "batch_normalization_155 (Bat (None, 220, 220, 16)      64        \n",
            "_________________________________________________________________\n",
            "activation_155 (Activation)  (None, 220, 220, 16)      0         \n",
            "_________________________________________________________________\n",
            "dropout_156 (Dropout)        (None, 220, 220, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_168 (Conv2D)          (None, 216, 216, 16)      2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_156 (Bat (None, 216, 216, 16)      64        \n",
            "_________________________________________________________________\n",
            "activation_156 (Activation)  (None, 216, 216, 16)      0         \n",
            "_________________________________________________________________\n",
            "dropout_157 (Dropout)        (None, 216, 216, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_169 (Conv2D)          (None, 212, 212, 32)      4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_157 (Bat (None, 212, 212, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_157 (Activation)  (None, 212, 212, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_158 (Dropout)        (None, 212, 212, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_170 (Conv2D)          (None, 208, 208, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_158 (Bat (None, 208, 208, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_158 (Activation)  (None, 208, 208, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_159 (Dropout)        (None, 208, 208, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_171 (Conv2D)          (None, 204, 204, 48)      13872     \n",
            "_________________________________________________________________\n",
            "batch_normalization_159 (Bat (None, 204, 204, 48)      192       \n",
            "_________________________________________________________________\n",
            "activation_159 (Activation)  (None, 204, 204, 48)      0         \n",
            "_________________________________________________________________\n",
            "dropout_160 (Dropout)        (None, 204, 204, 48)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_172 (Conv2D)          (None, 200, 200, 48)      20784     \n",
            "_________________________________________________________________\n",
            "batch_normalization_160 (Bat (None, 200, 200, 48)      192       \n",
            "_________________________________________________________________\n",
            "activation_160 (Activation)  (None, 200, 200, 48)      0         \n",
            "_________________________________________________________________\n",
            "dropout_161 (Dropout)        (None, 200, 200, 48)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_173 (Conv2D)          (None, 196, 196, 64)      27712     \n",
            "_________________________________________________________________\n",
            "batch_normalization_161 (Bat (None, 196, 196, 64)      256       \n",
            "_________________________________________________________________\n",
            "activation_161 (Activation)  (None, 196, 196, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_162 (Dropout)        (None, 196, 196, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_174 (Conv2D)          (None, 192, 192, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_162 (Bat (None, 192, 192, 64)      256       \n",
            "_________________________________________________________________\n",
            "activation_162 (Activation)  (None, 192, 192, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_163 (Dropout)        (None, 192, 192, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_175 (Conv2D)          (None, 188, 188, 96)      55392     \n",
            "_________________________________________________________________\n",
            "batch_normalization_163 (Bat (None, 188, 188, 96)      384       \n",
            "_________________________________________________________________\n",
            "activation_163 (Activation)  (None, 188, 188, 96)      0         \n",
            "_________________________________________________________________\n",
            "dropout_164 (Dropout)        (None, 188, 188, 96)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_176 (Conv2D)          (None, 184, 184, 96)      83040     \n",
            "_________________________________________________________________\n",
            "batch_normalization_164 (Bat (None, 184, 184, 96)      384       \n",
            "_________________________________________________________________\n",
            "activation_164 (Activation)  (None, 184, 184, 96)      0         \n",
            "_________________________________________________________________\n",
            "dropout_165 (Dropout)        (None, 184, 184, 96)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_177 (Conv2D)          (None, 180, 180, 128)     110720    \n",
            "_________________________________________________________________\n",
            "batch_normalization_165 (Bat (None, 180, 180, 128)     512       \n",
            "_________________________________________________________________\n",
            "activation_165 (Activation)  (None, 180, 180, 128)     0         \n",
            "_________________________________________________________________\n",
            "dropout_166 (Dropout)        (None, 180, 180, 128)     0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_10  (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "age_output (Dense)           (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 368,309\n",
            "Trainable params: 367,029\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6zJovVFaA0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or \n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "    \n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```    \n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore \n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where \n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored \n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on \n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKQmRs7xaBXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clr = CyclicLR(base_lr=1e-12, max_lr=.1, step_size=14421)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saqZSwPChDqz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "c6cce3d6-4d42-4352-c6ec-c9b2fd1c8ff7"
      },
      "source": [
        "# clr = CyclicLR(base_lr=0.000001, max_lr=.1, step_size=5776)\n",
        "model2 = get_compiled_model()\n",
        "train_gen = DataGenerator(train_df, batch_size=BATCH_SIZE, shuffle=True)\n",
        "history = model2.fit_generator(\n",
        "    generator=train_gen,\n",
        "    # validation_data=valid_gen,\n",
        "    epochs=100,\n",
        "    # class_weight={\n",
        "    #     0: 1.15,\n",
        "    #     1: 0.49,\n",
        "    #     2: 0.82,\n",
        "    #     3: 1.65,\n",
        "    #     4: 3.88\n",
        "    # },\n",
        "    # callbacks=[clr]\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "267/722 [==========>...................] - ETA: 3:43 - loss: 1.4530 - acc: 0.3881"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-af88e2a7b855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# validation_data=valid_gen,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# class_weight={\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#     0: 1.15,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7gYXJkzvhew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "425361f1-b6d8-4481-b7bc-6adff48ec8a2"
      },
      "source": [
        "print(keras.__version__)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkTE4odNerb2",
        "colab_type": "code",
        "outputId": "d6cebd5a-a154-4d2f-8fb2-1c9ee9da35c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    Dropout\n",
        ")\n",
        "from keras.layers import (\n",
        "    Convolution2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D,\n",
        "    GlobalAveragePooling2D\n",
        ")\n",
        "from keras.layers import add\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "regularization_strength=0.0001\n",
        "\n",
        "\n",
        "backbone = Sequential()\n",
        "# 1st set of Conv + Relu\n",
        "backbone.add(Convolution2D(16, (3, 3),dilation_rate=(2, 2),input_shape=(224, 224, 3))) #220\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 2nd set of Conv + Relu\n",
        "backbone.add(Convolution2D(16, (3, 3),strides=(2,2), kernel_regularizer=l2(regularization_strength))) #110\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 3rd set of Conv + Relu\n",
        "backbone.add(Convolution2D(32, (3, 3),dilation_rate=(2, 2), kernel_regularizer=l2(regularization_strength))) #106\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 4th set of Conv + Relu\n",
        "backbone.add(Convolution2D(32, 3, 3, kernel_regularizer=l2(regularization_strength))) #104\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 5th set of Conv + Relu\n",
        "backbone.add(Convolution2D(48, (3, 3),strides=(2,2), kernel_regularizer=l2(regularization_strength))) #52\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 7th set of Conv + Relu\n",
        "backbone.add(Convolution2D(48, (3, 3),dilation_rate=(2, 2), kernel_regularizer=l2(regularization_strength))) #48\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 8th set of Conv + Relu\n",
        "backbone.add(Convolution2D(64, 3, 3, kernel_regularizer=l2(regularization_strength))) #46\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 9th set of Conv + Relu\n",
        "backbone.add(Convolution2D(64, (3, 3),strides=(2,2), kernel_regularizer=l2(regularization_strength))) #23\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 10th set of Conv + Relu\n",
        "backbone.add(Convolution2D(96, (3, 3),dilation_rate=(2, 2), kernel_regularizer=l2(regularization_strength))) #19\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 11th set of Conv + Relu\n",
        "backbone.add(Convolution2D(96, (3, 3),strides=(2,2), kernel_regularizer=l2(regularization_strength))) #9\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "# 12th set of Conv + Relu\n",
        "backbone.add(Convolution2D(128, (3, 3),dilation_rate=(2, 2), kernel_regularizer=l2(regularization_strength))) #5\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "\n",
        "backbone.add(Convolution2D(128, (3, 3), padding='same', kernel_regularizer=l2(regularization_strength)))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "\n",
        "backbone.add(Convolution2D(128, (3, 3), padding='same', kernel_regularizer=l2(regularization_strength)))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "\n",
        "backbone.add(Convolution2D(128, (3, 3), padding='same', kernel_regularizer=l2(regularization_strength)))\n",
        "backbone.add(BatchNormalization())\n",
        "backbone.add(Activation('relu'))\n",
        "backbone.add(Dropout(0.2))\n",
        "\n",
        "backbone.add(GlobalAveragePooling2D(name='avg_pool'))\n",
        "neck = backbone.output\n",
        "# neck = Flatten(name=\"flatten\")(neck)\n",
        "# neck = Dense(512, activation=\"relu\")(neck)\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"imagequality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"bodypose\", build_tower(neck))\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=gender\n",
        "    # outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_301_input (InputLayer (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_301 (Conv2D)          (None, 220, 220, 16)      448       \n",
            "_________________________________________________________________\n",
            "batch_normalization_288 (Bat (None, 220, 220, 16)      64        \n",
            "_________________________________________________________________\n",
            "activation_288 (Activation)  (None, 220, 220, 16)      0         \n",
            "_________________________________________________________________\n",
            "dropout_369 (Dropout)        (None, 220, 220, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_302 (Conv2D)          (None, 109, 109, 16)      2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_289 (Bat (None, 109, 109, 16)      64        \n",
            "_________________________________________________________________\n",
            "activation_289 (Activation)  (None, 109, 109, 16)      0         \n",
            "_________________________________________________________________\n",
            "dropout_370 (Dropout)        (None, 109, 109, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_303 (Conv2D)          (None, 105, 105, 32)      4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_290 (Bat (None, 105, 105, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_290 (Activation)  (None, 105, 105, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_371 (Dropout)        (None, 105, 105, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_304 (Conv2D)          (None, 103, 103, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_291 (Bat (None, 103, 103, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_291 (Activation)  (None, 103, 103, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_372 (Dropout)        (None, 103, 103, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_305 (Conv2D)          (None, 51, 51, 48)        13872     \n",
            "_________________________________________________________________\n",
            "batch_normalization_292 (Bat (None, 51, 51, 48)        192       \n",
            "_________________________________________________________________\n",
            "activation_292 (Activation)  (None, 51, 51, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_373 (Dropout)        (None, 51, 51, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_306 (Conv2D)          (None, 47, 47, 48)        20784     \n",
            "_________________________________________________________________\n",
            "batch_normalization_293 (Bat (None, 47, 47, 48)        192       \n",
            "_________________________________________________________________\n",
            "activation_293 (Activation)  (None, 47, 47, 48)        0         \n",
            "_________________________________________________________________\n",
            "dropout_374 (Dropout)        (None, 47, 47, 48)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_307 (Conv2D)          (None, 45, 45, 64)        27712     \n",
            "_________________________________________________________________\n",
            "batch_normalization_294 (Bat (None, 45, 45, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_294 (Activation)  (None, 45, 45, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_375 (Dropout)        (None, 45, 45, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_308 (Conv2D)          (None, 22, 22, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_295 (Bat (None, 22, 22, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_295 (Activation)  (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_376 (Dropout)        (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_309 (Conv2D)          (None, 18, 18, 96)        55392     \n",
            "_________________________________________________________________\n",
            "batch_normalization_296 (Bat (None, 18, 18, 96)        384       \n",
            "_________________________________________________________________\n",
            "activation_296 (Activation)  (None, 18, 18, 96)        0         \n",
            "_________________________________________________________________\n",
            "dropout_377 (Dropout)        (None, 18, 18, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_310 (Conv2D)          (None, 8, 8, 96)          83040     \n",
            "_________________________________________________________________\n",
            "batch_normalization_297 (Bat (None, 8, 8, 96)          384       \n",
            "_________________________________________________________________\n",
            "activation_297 (Activation)  (None, 8, 8, 96)          0         \n",
            "_________________________________________________________________\n",
            "dropout_378 (Dropout)        (None, 8, 8, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_311 (Conv2D)          (None, 4, 4, 128)         110720    \n",
            "_________________________________________________________________\n",
            "batch_normalization_298 (Bat (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_298 (Activation)  (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_379 (Dropout)        (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_312 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_299 (Bat (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_299 (Activation)  (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_380 (Dropout)        (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_313 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_300 (Bat (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_300 (Activation)  (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_381 (Dropout)        (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_314 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_301 (Bat (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_301 (Activation)  (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_382 (Dropout)        (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "avg_pool (GlobalAveragePooli (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_383 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_384 (Dropout)        (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "gender_output (Dense)        (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 845,234\n",
            "Trainable params: 843,186\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsWK2a4ygVqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raRvBddjgKRT",
        "colab_type": "code",
        "outputId": "9e2f12f8-35ff-4f78-b03c-b59e3a589cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "train_gen = DataGenerator(train_df, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_gen = DataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False)\n",
        "history = model.fit_generator(\n",
        "  generator=train_gen,\n",
        "  validation_data=val_gen,\n",
        "  epochs=20\n",
        ")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "722/722 [==============================] - 64s 89ms/step - loss: 0.7572 - acc: 0.5788 - val_loss: 0.7396 - val_acc: 0.6066\n",
            "Epoch 2/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.7157 - acc: 0.6182 - val_loss: 1.0618 - val_acc: 0.5702\n",
            "Epoch 3/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.6844 - acc: 0.6419 - val_loss: 0.7045 - val_acc: 0.6336\n",
            "Epoch 4/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.6611 - acc: 0.6552 - val_loss: 0.7424 - val_acc: 0.6385\n",
            "Epoch 5/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.6570 - acc: 0.6610 - val_loss: 0.6730 - val_acc: 0.6640\n",
            "Epoch 6/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.6456 - acc: 0.6685 - val_loss: 0.6314 - val_acc: 0.6793\n",
            "Epoch 7/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.6453 - acc: 0.6698 - val_loss: 0.7061 - val_acc: 0.6380\n",
            "Epoch 8/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.6437 - acc: 0.6803 - val_loss: 0.6560 - val_acc: 0.6655\n",
            "Epoch 9/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.6278 - acc: 0.6949 - val_loss: 0.6433 - val_acc: 0.7058\n",
            "Epoch 10/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.6144 - acc: 0.7137 - val_loss: 0.6169 - val_acc: 0.7186\n",
            "Epoch 11/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.6002 - acc: 0.7284 - val_loss: 0.6617 - val_acc: 0.6758\n",
            "Epoch 12/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5860 - acc: 0.7346 - val_loss: 0.6013 - val_acc: 0.7387\n",
            "Epoch 13/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5751 - acc: 0.7480 - val_loss: 0.6011 - val_acc: 0.7367\n",
            "Epoch 14/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.5549 - acc: 0.7629 - val_loss: 0.6822 - val_acc: 0.7019\n",
            "Epoch 15/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5519 - acc: 0.7691 - val_loss: 0.6901 - val_acc: 0.6577\n",
            "Epoch 16/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5372 - acc: 0.7820 - val_loss: 0.5351 - val_acc: 0.7760\n",
            "Epoch 17/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5290 - acc: 0.7887 - val_loss: 0.4999 - val_acc: 0.7942\n",
            "Epoch 18/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5247 - acc: 0.7920 - val_loss: 0.5356 - val_acc: 0.7991\n",
            "Epoch 19/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5228 - acc: 0.7863 - val_loss: 0.5824 - val_acc: 0.7220\n",
            "Epoch 20/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5087 - acc: 0.7942 - val_loss: 0.5484 - val_acc: 0.7623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8XaLewR-2pY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "de216e3c-c755-4dbd-8310-9decbd20f6de"
      },
      "source": [
        "history2 = model.fit_generator(\n",
        "  generator=train_gen,\n",
        "  validation_data=val_gen,\n",
        "  epochs=20\n",
        ")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5128 - acc: 0.7973 - val_loss: 0.5433 - val_acc: 0.7687\n",
            "Epoch 2/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.5001 - acc: 0.8017 - val_loss: 0.5424 - val_acc: 0.7795\n",
            "Epoch 3/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4921 - acc: 0.8088 - val_loss: 0.4773 - val_acc: 0.8124\n",
            "Epoch 4/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4851 - acc: 0.8163 - val_loss: 0.6429 - val_acc: 0.7662\n",
            "Epoch 5/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4815 - acc: 0.8177 - val_loss: 0.5296 - val_acc: 0.8026\n",
            "Epoch 6/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4764 - acc: 0.8177 - val_loss: 0.4830 - val_acc: 0.8188\n",
            "Epoch 7/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4666 - acc: 0.8248 - val_loss: 0.5742 - val_acc: 0.7746\n",
            "Epoch 8/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4658 - acc: 0.8257 - val_loss: 0.4916 - val_acc: 0.8217\n",
            "Epoch 9/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4669 - acc: 0.8254 - val_loss: 0.4957 - val_acc: 0.7913\n",
            "Epoch 10/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4701 - acc: 0.8217 - val_loss: 0.5229 - val_acc: 0.7721\n",
            "Epoch 11/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4559 - acc: 0.8334 - val_loss: 0.4852 - val_acc: 0.8276\n",
            "Epoch 12/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4511 - acc: 0.8328 - val_loss: 0.5115 - val_acc: 0.7908\n",
            "Epoch 13/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4544 - acc: 0.8404 - val_loss: 0.4574 - val_acc: 0.8310\n",
            "Epoch 14/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4497 - acc: 0.8389 - val_loss: 0.4446 - val_acc: 0.8320\n",
            "Epoch 15/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4445 - acc: 0.8420 - val_loss: 0.4611 - val_acc: 0.8320\n",
            "Epoch 16/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4405 - acc: 0.8445 - val_loss: 0.4729 - val_acc: 0.8247\n",
            "Epoch 17/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4341 - acc: 0.8496 - val_loss: 0.4925 - val_acc: 0.8134\n",
            "Epoch 18/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4393 - acc: 0.8443 - val_loss: 0.5732 - val_acc: 0.7701\n",
            "Epoch 19/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4325 - acc: 0.8484 - val_loss: 0.4651 - val_acc: 0.8222\n",
            "Epoch 20/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4290 - acc: 0.8506 - val_loss: 0.4683 - val_acc: 0.8296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2OH3Q9kLeqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "ad0b4796-5d28-43f8-a9bf-3f8712a68a60"
      },
      "source": [
        "history3 = model.fit_generator(\n",
        "  generator=train_gen,\n",
        "  validation_data=val_gen,\n",
        "  epochs=20\n",
        ")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "722/722 [==============================] - 51s 70ms/step - loss: 0.4315 - acc: 0.8502 - val_loss: 0.4918 - val_acc: 0.8188\n",
            "Epoch 2/20\n",
            "722/722 [==============================] - 50s 70ms/step - loss: 0.4304 - acc: 0.8528 - val_loss: 0.4512 - val_acc: 0.8296\n",
            "Epoch 3/20\n",
            "722/722 [==============================] - 50s 69ms/step - loss: 0.4286 - acc: 0.8546 - val_loss: 0.4766 - val_acc: 0.8129\n",
            "Epoch 4/20\n",
            "722/722 [==============================] - 49s 68ms/step - loss: 0.4234 - acc: 0.8543 - val_loss: 0.4630 - val_acc: 0.8389\n",
            "Epoch 5/20\n",
            "722/722 [==============================] - 49s 68ms/step - loss: 0.4194 - acc: 0.8590 - val_loss: 0.4675 - val_acc: 0.8306\n",
            "Epoch 6/20\n",
            "722/722 [==============================] - 49s 67ms/step - loss: 0.4205 - acc: 0.8597 - val_loss: 0.4620 - val_acc: 0.8325\n",
            "Epoch 7/20\n",
            "722/722 [==============================] - 49s 67ms/step - loss: 0.4212 - acc: 0.8607 - val_loss: 0.5070 - val_acc: 0.8060\n",
            "Epoch 8/20\n",
            "722/722 [==============================] - 49s 68ms/step - loss: 0.4081 - acc: 0.8676 - val_loss: 0.4660 - val_acc: 0.8389\n",
            "Epoch 9/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.4103 - acc: 0.8638 - val_loss: 0.5012 - val_acc: 0.8369\n",
            "Epoch 10/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.4070 - acc: 0.8677 - val_loss: 0.4423 - val_acc: 0.8458\n",
            "Epoch 11/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.4060 - acc: 0.8672 - val_loss: 0.4628 - val_acc: 0.8315\n",
            "Epoch 12/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.4050 - acc: 0.8706 - val_loss: 0.4592 - val_acc: 0.8340\n",
            "Epoch 13/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.4102 - acc: 0.8663 - val_loss: 0.4427 - val_acc: 0.8527\n",
            "Epoch 14/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.4014 - acc: 0.8746 - val_loss: 0.5356 - val_acc: 0.8104\n",
            "Epoch 15/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3957 - acc: 0.8766 - val_loss: 0.5142 - val_acc: 0.8178\n",
            "Epoch 16/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3990 - acc: 0.8752 - val_loss: 0.4432 - val_acc: 0.8448\n",
            "Epoch 17/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3986 - acc: 0.8719 - val_loss: 0.4657 - val_acc: 0.8330\n",
            "Epoch 18/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.4014 - acc: 0.8723 - val_loss: 0.5253 - val_acc: 0.8291\n",
            "Epoch 19/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3916 - acc: 0.8778 - val_loss: 0.4570 - val_acc: 0.8399\n",
            "Epoch 20/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3950 - acc: 0.8759 - val_loss: 0.4394 - val_acc: 0.8561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3Vai5nbPTDe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "e436795d-488f-4212-aca5-fd0f156b4e9e"
      },
      "source": [
        "history4 = model.fit_generator(\n",
        "  generator=train_gen,\n",
        "  validation_data=val_gen,\n",
        "  epochs=20\n",
        ")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3999 - acc: 0.8731 - val_loss: 0.4758 - val_acc: 0.8409\n",
            "Epoch 2/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3842 - acc: 0.8831 - val_loss: 0.4591 - val_acc: 0.8472\n",
            "Epoch 3/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3857 - acc: 0.8839 - val_loss: 0.4704 - val_acc: 0.8468\n",
            "Epoch 4/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3920 - acc: 0.8812 - val_loss: 0.4612 - val_acc: 0.8472\n",
            "Epoch 5/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3827 - acc: 0.8839 - val_loss: 0.4903 - val_acc: 0.8296\n",
            "Epoch 6/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3924 - acc: 0.8808 - val_loss: 0.4975 - val_acc: 0.8193\n",
            "Epoch 7/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3803 - acc: 0.8873 - val_loss: 0.4495 - val_acc: 0.8522\n",
            "Epoch 8/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3862 - acc: 0.8867 - val_loss: 0.4607 - val_acc: 0.8355\n",
            "Epoch 9/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3854 - acc: 0.8897 - val_loss: 0.4402 - val_acc: 0.8595\n",
            "Epoch 10/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3780 - acc: 0.8901 - val_loss: 0.4633 - val_acc: 0.8531\n",
            "Epoch 11/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3785 - acc: 0.8899 - val_loss: 0.5025 - val_acc: 0.8369\n",
            "Epoch 12/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3820 - acc: 0.8872 - val_loss: 0.4604 - val_acc: 0.8590\n",
            "Epoch 13/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3776 - acc: 0.8888 - val_loss: 0.4664 - val_acc: 0.8497\n",
            "Epoch 14/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3720 - acc: 0.8927 - val_loss: 0.4624 - val_acc: 0.8458\n",
            "Epoch 15/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3800 - acc: 0.8917 - val_loss: 0.5083 - val_acc: 0.8325\n",
            "Epoch 16/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3749 - acc: 0.8921 - val_loss: 0.4608 - val_acc: 0.8590\n",
            "Epoch 17/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3730 - acc: 0.8968 - val_loss: 0.4927 - val_acc: 0.8374\n",
            "Epoch 18/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3744 - acc: 0.8950 - val_loss: 0.4728 - val_acc: 0.8404\n",
            "Epoch 19/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3683 - acc: 0.8986 - val_loss: 0.4942 - val_acc: 0.8227\n",
            "Epoch 20/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3741 - acc: 0.8952 - val_loss: 0.4710 - val_acc: 0.8492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-qIq8aBTCeZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "20084fa5-22b3-475e-9812-5e881666b013"
      },
      "source": [
        "history5 = model.fit_generator(\n",
        "  generator=train_gen,\n",
        "  validation_data=val_gen,\n",
        "  epochs=20\n",
        ")"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3630 - acc: 0.9016 - val_loss: 0.5156 - val_acc: 0.8281\n",
            "Epoch 2/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3670 - acc: 0.8994 - val_loss: 0.5012 - val_acc: 0.8291\n",
            "Epoch 3/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3717 - acc: 0.8993 - val_loss: 0.5174 - val_acc: 0.8266\n",
            "Epoch 4/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3763 - acc: 0.8962 - val_loss: 0.4917 - val_acc: 0.8325\n",
            "Epoch 5/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3677 - acc: 0.8995 - val_loss: 0.4821 - val_acc: 0.8399\n",
            "Epoch 6/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3848 - acc: 0.8893 - val_loss: 0.4750 - val_acc: 0.8522\n",
            "Epoch 7/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3808 - acc: 0.8947 - val_loss: 0.6260 - val_acc: 0.7878\n",
            "Epoch 8/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3708 - acc: 0.9006 - val_loss: 0.4755 - val_acc: 0.8527\n",
            "Epoch 9/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3699 - acc: 0.9000 - val_loss: 0.4742 - val_acc: 0.8453\n",
            "Epoch 10/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3695 - acc: 0.8992 - val_loss: 0.4950 - val_acc: 0.8512\n",
            "Epoch 11/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3771 - acc: 0.8976 - val_loss: 0.4624 - val_acc: 0.8595\n",
            "Epoch 12/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3682 - acc: 0.8998 - val_loss: 0.5099 - val_acc: 0.8418\n",
            "Epoch 13/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3649 - acc: 0.9005 - val_loss: 0.4960 - val_acc: 0.8531\n",
            "Epoch 14/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3608 - acc: 0.9031 - val_loss: 0.5023 - val_acc: 0.8477\n",
            "Epoch 15/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3656 - acc: 0.9043 - val_loss: 0.4721 - val_acc: 0.8458\n",
            "Epoch 16/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3667 - acc: 0.9022 - val_loss: 0.4835 - val_acc: 0.8551\n",
            "Epoch 17/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3602 - acc: 0.9043 - val_loss: 0.4907 - val_acc: 0.8517\n",
            "Epoch 18/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3645 - acc: 0.9059 - val_loss: 0.5075 - val_acc: 0.8374\n",
            "Epoch 19/20\n",
            "722/722 [==============================] - 48s 67ms/step - loss: 0.3738 - acc: 0.9005 - val_loss: 0.4693 - val_acc: 0.8620\n",
            "Epoch 20/20\n",
            "722/722 [==============================] - 48s 66ms/step - loss: 0.3733 - acc: 0.9026 - val_loss: 0.4806 - val_acc: 0.8517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoBKk9Lrea8K",
        "colab_type": "code",
        "outputId": "bfb939a5-37e5-447a-bf5e-8d0aa0614352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_pairs(metric, val_metric):\n",
        "  plt.plot(range(len(metric)), metric, color='blue')\n",
        "  plt.plot(range(len(val_metric)), val_metric, color='green')\n",
        "  plt.show()\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plot_pairs(clr.history['lr'], [])\n",
        "plot_pairs(loss, val_loss)\n",
        "plot_pairs(acc, val_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAQlAxERAf4fJ9YOICuvJmAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXiU5b3/8fc3KyFA2MK+KyIBwmKK\n7BiU3cF9QTKktv0p1baItj/Pr5unp/W6zmmreDzaKi0ekUWrgoiIgrIoKKBhCUvY900IENaErN/f\nH09AxCQzSWYyS76v65prkpn7eZ5vED+5uee571tUFWOMMaEvItAFGGOM8Q0LdGOMCRMW6MYYEyYs\n0I0xJkxYoBtjTJiICtSFmzZtqh06dAjU5Y0xJiStW7fupKomlvVewAK9Q4cOZGRkBOryxhgTkkTk\nQHnv2ZCLMcaECQt0Y4wJExboxhgTJizQjTEmTFigG2NMmLBAN8aYMGGBbowxYSLkAj0rC558Ei5d\nCnQlxhgTXEIu0Pfvh6lTYcWKQFdijDHBJeQCfdgwqFsXPvgg0JUYY0xwCblAr1MHhg93At02WzLG\nmG+FXKADuFxw6BBs2hToSowxJnh4DHQRaSsiy0UkS0S2isjkMtokiMgHIpJZ2uZh/5TrGDvWebZh\nF2OM+ZY3PfQi4ClVTQL6AY+LSNI1bR4HslS1J3AL8JyIxPi00qu0aAF9+1qgG2PM1TwGuqoeU9X1\npV+fB7YBra9tBtQXEQHqAadxfhH4jcsFX30Fx4758yrGGBM6KjWGLiIdgN7A2mveegnoChwFNgOT\nVbWkjOMfEZEMEcnIzs6uUsGXuVzO84cfVus0xhgTNrwOdBGpB8wFnlDVc9e8PRLYCLQCegEviUiD\na8+hqtNUNUVVUxITy9xww2vJydCunQ27GGPMZV4FuohE44T5bFWdV0aTh4F56tgN7ANu9F2ZZdXk\n9NI/+QTy8vx5JWOMCQ3e3OUiwHRgm6o+X06zg8Ctpe2bA12Avb4qsjwulxPmy5b5+0rGGBP8vOmh\nDwTcwDAR2Vj6GCMik0RkUmmbPwIDRGQzsBR4WlVP+qnmK265BerVs2EXY4wBLzaJVtVVgHhocxQY\n4auivBUbCyNGwMKFzqxRqbBKY4wJbyE5U/RqLhccOQIbNgS6EmOMCayQD/QxY5ye+YIFga7EGGMC\nK+QDvVkz6NfPxtGNMSbkAx2cYZf1652hF2OMqa3CItDHjXOeFy4MbB3GGBNIYRHoSUnQsaMNuxhj\narewCPTLs0aXLoXc3EBXY4wxgREWgQ5OoF+6BJ9+GuhKjDEmMMIm0IcMgQYNbNjFGFN7hU2gx8TA\nyJFOoJd8b+FeY4wJf2ET6OAMuxw/DhkZga7EGGNqXlgF+pgxEBFhwy7GmNoprAK9SRMYONAC3RhT\nO4VVoIMz7JKZCQcPBroSY4ypWWEZ6GCzRo0xtU/YBXqXLnD99TbsYoypfcIu0C/PGl22DC5cCHQ1\nxhhTc8Iu0MEJ9IICWLIk0JUYY0zNCctAHzQIEhJs2MUYU7uEZaBHR8Po0fDhh1BcHOhqjDGmZoRl\noIOzRnp2Nnz1VaArMcaYmhG2gT5qFERG2rCLMab28BjoItJWRJaLSJaIbBWRyeW0u0VENpa2+cz3\npVZOo0YweLAFujGm9vCmh14EPKWqSUA/4HERSbq6gYg0BP4GjFPVbsB9Pq+0Clwu2LIF9u8PdCXG\nGON/HgNdVY+p6vrSr88D24DW1zR7CJinqgdL253wdaFVcXnWqPXSjTG1QaXG0EWkA9AbWHvNWzcA\njURkhYisE5GJvimvejp3dmaOLlgQ6EqMMcb/vA50EakHzAWeUNVz17wdBdwEjAVGAr8TkRvKOMcj\nIpIhIhnZ2dnVKNt7Lhd89hmcu7ZiY4wJM14FuohE44T5bFWdV0aTw8BiVb2oqieBz4Ge1zZS1Wmq\nmqKqKYmJidWp22suFxQWwuLFNXI5Y4wJGG/uchFgOrBNVZ8vp9n7wCARiRKRusDNOGPtATdgADRu\nbOPoxpjwF+VFm4GAG9gsIhtLX/s10A5AVV9R1W0i8jGwCSgB/qmqW/xRcGVFRTk7GS1a5MwajYwM\ndEXGGOMfHgNdVVcB4kW7vwB/8UVRvuZywaxZsHq1s86LMcaEo7CdKXq1kSOdnroNuxhjwlmtCPSE\nBBg61ALdGBPeakWggzPssm0b7N4d6EqMMcY/Qi7QNxzbwMT3JpJXmFep42zWqDEm3IVcoJ/KO8XM\nTTNZuLNyu0B36gRJSRboxpjwFXKBntohlVb1WzFz08xKHztuHKxcCWfO+KEwY4wJsJAL9MiISCb0\nmMBHuz8i+2Lllg9wuaCoCD7+2E/FGWNMAIVcoAOkJadRVFLE21vfrtRxN98MTZvasIsxJjyFZKAn\nN08muXlypYddIiNh7Fj46COnp26MMeEkJAMdwJ3sZu2Rtew8tbNSx7lckJMDX3zhp8KMMSZAQjbQ\nx3cfjyDM3jS7UseNGAExMbZGujEm/IRsoLdu0JpbO93KrM2zUFWvj6tfH265xcbRjTHhJ2QDHZxh\nl705e/ny0JeVOs7lgl27YMcOPxVmjDEBENKBfteNdxEXFcesTbMqdZzNGjXGhKOQDvT6sfW5q+td\n/Gvrv8gvyvf6uPbtITnZAt0YE15COtDBGXbJuZTDol2LKnWcy+Xc6XL6tJ8KM8aYGhbygX5bp9to\nHt+cWZsrP+xSXOzck26MMeEg5AM9KiKK8d3Hs3DnQnLycrw+7gc/gObNbdjFGBM+Qj7QAdw93RQU\nF1RqKYCIiG9njRYU+LE4Y4ypIWER6L1b9KZr065VGnY5d85ZgdEYY0JdWAS6iOBOdrPq4Cr25ezz\n+rjhwyE21oZdjDHhISwCHWBC8gSASt2THh8Pt97qBHolJpsaY0xQCptAb5fQjqHth1Z6KQCXC/bu\ndfYbNcaYUOYx0EWkrYgsF5EsEdkqIpMraPsDESkSkXt9W6Z33Mludp7ayddHv/b6mNtvd55t2MUY\nE+q86aEXAU+pahLQD3hcRJKubSQikcB/AUt8W6L37k26lzpRdZiZ6f066W3aQO/eFujGmNDnMdBV\n9Ziqri/9+jywDWhdRtOfA3OBEz6tsBIS6iQwrss43tr6FoXFhV4f53LB6tWQXbkd7YwxJqhUagxd\nRDoAvYG117zeGrgL+LuH4x8RkQwRycj2U3qm9UjjZO5JFu9Z7PUxLheUlMCiyq0eYIwxQcXrQBeR\nejg98CdU9dw1b78APK2qJRWdQ1WnqWqKqqYkJiZWvlovjLp+FE3rNq3U9nR9+kDLljbsYowJbV4F\nuohE44T5bFWdV0aTFOAtEdkP3Av8TUTu9FmVlRAdGc2D3R5kwY4FnL101qtjIiKcD0cXL4Z87xdt\nNMaYoOLNXS4CTAe2qerzZbVR1Y6q2kFVOwDvAo+p6nyfVloJaclpXCq6xNxtc70+Ztw4uHABPvvM\nj4UZY4wfedNDHwi4gWEisrH0MUZEJonIJD/XVyV9W/elc+POlRp2ufVWiIuzYRdjTOiK8tRAVVcB\n4u0JVfWH1SnIFy4vBfD7Fb/n4NmDtEto5/GYuDi47TYn0F98EcTrn9gYY4JD2MwUvdblpQDmbJ7j\n9TEuFxw4AFu2+KsqY4zxn7AN9E6NOjGw7UBmbprp9VIANmvUGBPKwjbQwVkKICs7i43fbPSqfcuW\nkJICCxb4uTBjjPGDsA70+7rdR0xkTKU+HHW54Kuv4PhxPxZmjDF+ENaB3jiuMWM7j2XO5jkUlRR5\ndYzL5Syl++GHfi7OGGN8LKwDHZxhl+MXj7N071Kv2vfq5SzYZePoxphQE/aBPqbzGBrVaeT1sIuI\n00tfsgQuXfJzccYY40NhH+ixUbHc3+1+3tv+HhcKLnh1jMsFubmwfLmfizPGGB8K+0AHZ9gltzCX\nedvKWobm+1JTne3pbNjFGBNKakWgD2g7gI4NO3q932idOs4G0rbXqDEmlNSKQBcR0pLTWLpvKUfP\nH/XqGJcLDh+Gjd7dwm6MMQFXKwIdnBUYS7TE66UAxo51PiC1YRdjTKioNYF+Q5Mb6Nu6r9fDLs2b\nQ9++FujGmNBRawIdnA9HM49nsvn4Zq/ajxsHGRlw1LtRGmOMCahaFegPdHuAqIgor+9Jd7mcZ5s1\naowJBbUq0BPjExl1/SjmbJ5DcUmxx/bdu0P79jbsYowJDbUq0MEZdjly/ggr9q/w2PbyrNFPP4W8\nPP/XZowx1VHrAt11g4sGsQ0qNeySlwdLvVsKxhhjAqbWBXpcdBz3Jd3H3G1zyS3M9dh+6FCoV8/W\nSDfGBL9aF+jg3JN+oeAC729/32Pb2FgYORIWLoSSkhoozhhjqqhWBvqQ9kNol9CuUsMux47B+vV+\nLswYY6qhVgZ6hEQwoccEluxZwvELnrcmGjMGIiLsbhdjTHDzGOgi0lZElotIlohsFZHJZbSZICKb\nRGSziHwpIj39U67vpCWnUazFvLXlLY9tExOhf38LdGNMcPOmh14EPKWqSUA/4HERSbqmzT5gqKr2\nAP4ITPNtmb6XlJhEn5Z9KjXssmGDs2CXMcYEI4+BrqrHVHV96dfngW1A62vafKmqOaXfrgHa+LpQ\nf3Anu1l3bB3bsrd5bHt51ujChX4uyhhjqqhSY+gi0gHoDaytoNmPgY/KOf4REckQkYzs7OzKXNov\nHuz+IBES4dWCXV27QqdONuxijAleXge6iNQD5gJPqOq5ctqk4gT602W9r6rTVDVFVVMSExOrUq9P\ntajXghHXjWDW5lmUaMX3JF6eNbp0KVy8WEMFGmNMJXgV6CISjRPms1W1zH3cRCQZ+Cdwh6qe8l2J\n/uVOdnPw7EFWHVzlsa3LBfn58MknNVCYMcZUkjd3uQgwHdimqs+X06YdMA9wq+pO35boX3d0uYP4\n6HhmZnr+cHTwYGjQwIZdjDHByZse+kDADQwTkY2ljzEiMklEJpW2+T3QBPhb6fsZ/irY1+Jj4rkn\n6R7eyXqHS0WXKmwbEwOjRzvL6dqsUWNMsPHmLpdVqiqqmqyqvUofi1T1FVV9pbTNT1S10VXvp/i/\ndN9xJ7s5m3+WhTs938LicsHx4/D11zVQmDHGVEKtnCl6rdQOqbSq38qre9JHj4bISBt2McYEHwt0\nIDIikoe6P8SiXYs4mXuywraNG8PAgRboxpjgY4Feyt3TTVFJEW9vfdtjW5cLNm2CAwdqoDBjjPGS\nBXqp5ObJ9GjWw6thl8uzRq2XbowJJhboV3Enu1lzeA27Tu2qsF2XLtC5swW6MSa4WKBf5aEeDyEI\nszfP9tjW5YIVK+D8ef/XZYwx3rBAv0rrBq0Z1nEYszbNQlUrbDtuHBQUwJIlNVScMcZ4YIF+DXey\nmz05e1h9eHWF7QYOhEaNbNjFGBM8LNCvcXfXu4mLivO4AmNU1LezRouLa6g4Y4ypgAX6NerH1ufO\nG+/kX1v/RUFxQYVtXS44eRLWVrSYsDHG1BAL9DK4k92czjvNol2LKmw3apTTU7dhF2NMMLBAL8Pw\n64bTPL65x2GXhg2dFRgXLKihwowxpgIW6GWIiohifPfxfLDzA3Lycips63JBVhbs3VtDxRljTDks\n0MuRlpxGQXEB72S9U2E7mzVqjAkWFujl6NOyD12bdvU47HL99c5+oxboxphAs0Avh4jgTnaz8uBK\n9p/ZX2Fblws++wzOnq2Z2owxpiwW6BV4qMdDAB576S4XFBXB4sU1UZUxxpTNAr0C7Ru2Z2j7oR6X\nAujfH5o0sWEXY0xgWaB74E52s+PUDjKOlr9NamQkjBkDixY5PXVjjAkEC3QP7km6h9jIWI/rpLtc\ncPo0fPllDRVmjDHXsED3oGGdhozrMo63trxFYXFhue1GjoToaBt2McYEjgW6F9zJbrJzs1myp/y1\nchs0gKFDLdCNMYHjMdBFpK2ILBeRLBHZKiKTy2gjIvKiiOwWkU0i0sc/5QbGyOtH0iSuicdhl3Hj\nYMcO2FXxhkfGGOMX3vTQi4CnVDUJ6Ac8LiJJ17QZDXQufTwC/N2nVQZYTGQMD3Z/kPd3vM+5/HPl\ntrNZo8aYQPIY6Kp6TFXXl359HtgGtL6m2R3AG+pYAzQUkZY+rzaA3MluLhVdYm7W3HLbdOgA3btb\noBtjAqNSY+gi0gHoDVy7Anhr4NBV3x/m+6GPiDwiIhkikpGdnV25SgOsb+u+dG7c2au7XVauhJyK\n1/Qyxhif8zrQRaQeMBd4QlXLH3eogKpOU9UUVU1JTEysyikCRkRIS05jxf4VHDp7qNx2Lpezg9HH\nH9dgccYYg5eBLiLROGE+W1XnldHkCND2qu/blL4WVtKS01CUOZvnlNumb19ITLQ10o0xNc+bu1wE\nmA5sU9Xny2m2AJhYerdLP+Csqh7zYZ1BoVOjTgxoO4CZm2aWuxRAZCSMHQsffQSF5d+2bowxPudN\nD30g4AaGicjG0scYEZkkIpNK2ywC9gK7gX8Aj/mn3MBzJ7vZmr2VzOOZ5bZxuZyVF1etqsHCjDG1\nXpSnBqq6ChAPbRR43FdFBbP7u93PLz76BTMzZ9KrRa8y24wYATExzt0uqak1XKAxptaymaKV1Diu\nMWNvGMucLXMoKil7Ja569WDYMCfQK1ik0RhjfMoCvQrcyW6+ufANy/YtK7eNywW7dzszR40xpiZY\noFfB2M5jaVSnUYX3pN9+u/Nsk4yMMTXFAr0KYqNiub/b/czbNo8LBRfKbNOuHfTsaYFujKk5FuhV\nlJacRm5hLu9te6/cNi4XfPEFnDpVg4UZY2otC/QqGth2IB0bdmTW5vL3G3W5oKTE2cnIGGP8zQK9\nii4vBfDp3k85dr7sOVQpKdCqFTz7LBw+XMMFGmNqHQv0akhLTqNES8pdCiAiAubMgaNHYeBAu+PF\nGONfFujVcEOTG+jbum+Fwy5Dh8KKFZCXB4MGQUb5e00bY0y1WKBXkzvZzcZvNrLlxJZy2/Tp43w4\nWq+eM3N06dIaLNAYU2tYoFfTA90eICoiipmZFa+T3rmzE+odOsCYMfDuuzVTnzGm9rBAr6bE+ERG\nXT+K2ZtnU6IlFbZt1Qo+/9z5sPT++2HatBoq0hhTK1ig+4A72c2R80dYsX+Fx7aNGsEnn8CoUfDo\no84dMLbeizHGFyzQfcB1g4sGsQ08bk93Wd268P77MGEC/Pa38OSTzv3qxhhTHRboPhAXHce9Xe9l\nbtZccgtzvTomOhreeAMmT4YXXoD0dNsQwxhTPRboPuLu6eZ8wXkW7PB+77mICJg6Ff70J5g1C+66\nC3K9+31gjDHfY4HuI0PaD6Ftg7ZeD7tcJgK/+Q288oqzRMDw4ZCT46cijTFhzQLdRyIkggk9JrB4\n92JOXDxR6eMffRTeftuZeDRkiDO71BhjKsMC3YfcPd0UazFvbXmrSsffe6/TS9+/31kqYNcu39Zn\njAlvFug+lJSYRO8WvSs97HK1W2+F5cvhwgVnqYANG3xYoDEmrFmg+5g72U3G0Qy2n9xe5XOkpMCq\nVVCnzrdrwRhjjCcW6D42vsd4IiSCWZvKX7DLG126OEsFtG3rTEKaP99HBRpjwpbHQBeR10TkhIiU\nufqUiCSIyAcikikiW0XkYd+XGTpa1GvB8E7DmbVplselADxp08ZZKqB3b7jnHpg+3UdFGmPCkjc9\n9NeBURW8/ziQpao9gVuA50QkpvqlhS53spsDZw/wxcEvqn2uJk3g00+d2xl/8hP4r/+ypQKMMWXz\nGOiq+jlwuqImQH0REaBeadsi35QXmu688U7io+Or9eHo1eLjYcECGD8e/u3f4Fe/sqUCjDHf54sx\n9JeArsBRYDMwWbWaYw0hLj4mnru73s3bW9/mUtEln5wzJsaZTfqzn8Fzz8GPfmRLBRhjvssXgT4S\n2Ai0AnoBL4lIg7IaisgjIpIhIhnZ2dk+uHTwcie7OZt/lg93fuizc0ZEwIsvwr//O8yYAXff7eyE\nZIwx4JtAfxiYp47dwD7gxrIaquo0VU1R1ZTExEQfXDp4Des4jFb1W/ls2OUyEXjmGXj5ZfjwQxgx\nAs6c8ekljDEhyheBfhC4FUBEmgNdgL0+OG9Ii4yI5KHuD7Fo1yJO5Z7y+fkfewzefBPWrnXuVT92\nzOeXMMaEGG9uW3wTWA10EZHDIvJjEZkkIpNKm/wRGCAim4GlwNOqetJ/JYeOtOQ0CksKfd5Lv+yB\nB2DhQtizx5lVumePXy5jjAkRogG6By4lJUUzMjICcu2aNGD6ANYcXsOvBvyKP6T+gTpRdXx+jbVr\nnX1Ko6Nh8WLo2dPnlzDGBAkRWaeqKWW9ZzNF/Wxx2mJ+0ucn/PnLP5MyLYX1x9b7/Bo33+wsFRAd\n7Qy/rFzp80sYY0KABbqf1Y+tzzTXNBY9tIjTeae5+Z838x+f/QeFxb6957BrV2epgBYtnA9KP/jA\np6c3xoQAC/QaMrrzaLY8toX7u93PMyueYcBrA8jKzvLpNdq1c3rqPXo4ux+9/rpPT2+MCXIW6DWo\ncVxjZt89m3fue4d9Ofvo82ofnl/9PMUlxT67RtOmsHQppKbCww/DX//qs1MbY4KcBXoA3Jt0L1sf\n28rI60fy1JKnSJ2Ryt4c393pWb++c/fLffc5ywQ8/bSt/2JMbWCBHiDN6zVn/gPzef2O18k8nkny\n35N5NeNVfHXXUWysc5/6pEnw5z87C3sV1eoVdowJfxboASQipPdKZ8tPt9C/bX8mfTiJ0bNHc+Tc\nEZ+cPzIS/vY3+P3v4bXXnB77Jd8sLWOMCUIW6EGgbUJbFqct5uUxL7Py4Eq6/707szbN8klvXQT+\n8AdnDZj5853NMs6e9UHRxms7T+3k8Q8f56ZpN/HM8mfYc9pmgBn/sIlFQWbXqV388P0f8uWhL7m7\n6928MvYVEuN9s+7NnDmQng7du8PHH0Pz5j45rSmDqrJ8/3KmrpnKwp0LiYmMoVeLXnx95GsUZVC7\nQaT3TOe+pPtIqJMQ6HJNCKloYpEFehAqLinmudXP8bvlvyMhNoFprmnceeOdPjn3Rx85ux+1bg1L\nlkDHjj45rSmVX5TPm1ve5IU1L5B5PJPEuok89oPH+GnKT2lerzmHzh5i1qZZzMicwY5TO6gTVYe7\nbryL9J7p3NbpNiIjIgP9I5ggZ4Eeorac2MLE9yay4ZsNuJPdvDj6RRrWaVjt865eDWPHOptQL17s\n3Lduqif7YjavZLzCy1+/zPGLx+mW2I0p/aYwIXlCmcs9qCpfHfmKGZkzeGvLW+RcyqFV/Vak9Ugj\nvVc6SYlJAfgpTCiwQA9hBcUFPPv5szy78lla1GvBa3e8xojrRlT7vFu3OjNKc3OdWaWDBvmg2Fpo\n64mtvLDmBWZumkl+cT6jrx/NlH5TuK3TbTibeHmWX5TPBzs/YEbmDD7a9RHFWkxKqxTSe6Yzvvt4\nmtRt4uefwoQSC/Qw8PWRr5k4fyLbT27npyk/5c/D/0y9mHrVOuf+/U6oHzoE777r9NqNZ6rK4j2L\nmbpmKkv2LCEuKo6JPScy+ebJdE3sWq1zH79wnDmb5zAjcwaZxzOJjojm9htuJ71nOmM6jyE6MtpH\nP4UJVRboYSKvMI/fLvstU9dMpVOjTrx+5+sMale9rvWJEzB6NGRmwv/+L7jdPio2DOUV5jFz00xe\nWPMC205uo2W9lvys78949KZH/dKLzvwmkxmZM5i9eTYnLp6gad2mPNT9IdJ7pdO7RW+v/wVgwosF\nepj5/MDn/HD+D9l/Zj9P9X+KPw77Y7WW5T13zln7ZdkyePZZGDkSmjRxlhGIj3dufazNjp0/xstf\nv8wrGa9wKu8UvVv05sn+T3J/t/uJiYzx+/ULiwtZvGcxMzJnsGDHAgqKC+jerDvpPdOZ0GMCLeu3\nrN75C51dr86edZ4vP6KjoWFDSEj49rlBA2d+gwkcC/QwdKHgAr9c8kteXfcqSYlJvHHnG9zU6qYq\nn+/SJZgwAebN++7rMTFOsF8OeG+eGzQIj18CG7/ZyNQ1U3lz85sUlRQxrss4pvSbwpD2QwLWOz6d\nd5p/bfkXMzJnsPbIWiIkguEdR3L3den0b3QHeefrlBnOFT1ycytXQ/363w/6yjzXqRMefz8CxQI9\njH28+2N+vODHnLh4gt8O/i2/HvzrKo+zlpTA11/DN9/AqVNw8mT5z6dPO+3LEhXlBHtlfgk0bOhs\ngh1oJVrCwp0LmbpmKiv2ryA+Op4f9f4Rv7j5F1zf+Hq/XDM/v+LALS+cT7KdnHZvUJQ0ExIOw6UE\n2PIAZKbDof6Ak5qRkc6fr7ePhATnUVT03et7+1zsYa25snr+3j43SCghp+QAO05nsTV7K1nZWWRl\nZ5FzKYcBbQeQ2iGV1A6ptG/Y3i//rYKBBXqYy8nL4Rcf/4JZm2bRp2Uf3rjzDbo16+bXa5aUOP8D\nVxT6ZT2Xt55MRAQ0blx+4Jf1WuPGTlipOiFSVPT9R3mvX/s4n3+Bj469znvH/ptj+btpGt2OkQ1/\nztD6PyFWG3p1Dk/Xz88vO5w9LccQFVVxADdIKOZE/HI26AwyLs4jvySX9vU6c98NE3n4JjddW7av\nsR6xqtPjLyvovf2lcPEiIMXQaB8kboXErKse2yA679s/m7yWxOcmESv1OVN/FQXRzu6X8fmdSLyY\nSrOLqTTPSyW+pBUREVT5IVL1Y8s6V3Iy/OAHVfsztkCvJeZtm8ejCx/lfP55/jTsT0zpNyWoJqqo\nOuP1lfkFcPIkFBSUfb7L/5N56hFWqMEhuPl/oM8/IO4MHOoHa6bAtruhJKpSp4qOdn7BREWV/YiJ\n+ba3WZlHXJz3QxTn88/zbta7zMicwWcHPgMgtUMq6T3TuSfpnmrfGeVrRSVF7Dm95zu97a0nsthx\najv5xflX2jWJbkOLyCSalCSRkN+NuAtJROZ05VJOI86cgQsXoLikhNz4rZxruowLTZdzoelnlMSc\nASD6bBfijqdS51gqscduQXKbUVJCpR+q3z5Xx9NPw3/+Z9WOtUCvRU5cPMGjCx9l/vb5DGo3iNfv\neJ3rGl8X6LKqTNXpsZUX9sXF5QdoReG6N/8rFmQ/z6rT7wIwtNk9jO8whV5N+1V4XHmPYBguuta+\nnH3M3DSTNzLfYE/OHuKj4zpQQB4AAAonSURBVLkn6R7Se6ZzS4dbiJCaK7qguIDdp3ez9URpcJ90\nwnvHyR0Ulny7e1f7hPYkJSaRlJhEt8RuJCUm0TWxKw1iG1T6msUlxWQez2TZvmUs37+clQdWcr7g\nPADdm3W/MjwztMNQGsc1rtS5Vb8N96o86teHRo0q/SMBFui1jqoya9Msfv7RzyksKeSvw//KpJRJ\ntf42t6KSIuZvn8/UNVP58tCXJMQm8H/6/B9+1vdnYT3mqqp8cegLZmycwdtZb3Mu/xztEtrhTnaT\n3jOdzk06++xa+UX57Di140pv+/Jj1+ldFJU4422C0LFRx++EdlJiEjc2vdGv/4IoKili3dF1LN+/\nnGX7lrHq4CryivIQhF4tejkB3zGVwe0GB/X6OhbotdShs4f48YIf88neTxhx3Qimj5tOmwZtAl1W\njTt76SzTN0znxbUvcuDsATo16sTkmyfzcK+HqR9bP9Dl1ai8wjzmb5/PjMwZfLL3E0q0hP5t+pPe\nM50Huj/g9dISeYV5bD+5/dvQLu1x7z69mxJ1Pi2PkAiua3Qd3Zp1I6lp0pXg7tK0C3Wj6/rzx/RK\nQXEBXx35iuX7lrN8/3K+PPQl+cX5REgEN7W8iWEdh5HaIZVB7QYRHxMf6HKvsECvxVSVVzJe4Zef\n/JLoiGj+Z/T/kJacVit663tz9vLi2hd5bcNrnC84z5D2Q5jSbwquG1xB9dlCoBw9f/TKQmFZ2VnE\nRsZyx413kN4znRHXjSAqIoqLBRfZfnL7d8a4s7Kz2JuzF8XJjqiIKDo37nwlsC8/bmhyQ7XmR9S0\nS0WXWH1oNcv3OwG/9vBaCksKiYqIom/rvqR2SGVYx2H0b9OfuOi4gNVZrUAXkdeA24ETqtq9nDa3\nAC8A0cBJVR3qqSgL9Jq15/Qe0uen88WhL7jzxjt59fZXaRbfLNBl+ZyqsurgKqaumcr7O94nQiJ4\nsPuDTOk3hT4t+wS6vKCkqqw7to4ZG2fw5pY3OZV3iubxzYmLjmP/mf1X2kVHRNOlaRcnsK/qcXdu\n0rlGJljVtIsFF/ni0BdXevAZRzMo1mJiImPo36b/lSGam1vfTGxUbI3VVd1AHwJcAN4oK9BFpCHw\nJTBKVQ+KSDNVPeGpKAv0mldcUszUNVP5zbLf0CC2Aa/e/ip3d7070GX5RGFxIe9kvcPUNVPJOJpB\n47jGTLppEo/3fZxW9VsFuryQUVBcwKJdi3hzy5tESuR3etzXNbquVq8lcy7/HCsPrLzSg99wbAOK\nEhcVx8B2A698yJrSKsWvf07VHnIRkQ7AwnIC/TGglar+tjJFWaAHztYTW5k4fyLrj60nLTmNF0e9\nSKO4Kn7kHmCn804zbd00XvrqJY6cP0KXJl2Y0m8K7p7uoBinNeErJy+Hzw98fuUums0nNgNQL6Ye\ng9sNvtKD792it0+H+Pwd6JeHWroB9YH/VtU3yjnPI8AjAO3atbvpwIEDXv4IxtcKiwt5duWz/Onz\nP9G8XnNeG/caI68fWaM1qCoFxQVcLLxIbmEuFwsucrHw4pVnT6+duXSGxXsWk1uYy22dbmNKvymM\nun5Ujd6OZ8xl2Rez+ezAZ1cCfvvJ7QAkxCYwtMPQKz34Hs17VOvvqL8D/SUgBbgViANWA2NVdWdF\n57QeenBYd3QdE+dPJCs7i0dvepS/jvjrd24dKy4prlTIXv2cW+S5bbFWblZQTGQM8dHxxMfEEx8d\nz8C2A3mi3xP0aG67dJjgcuz8MVbsX3El4PfkOHvJNolrwq8H/5on+z9ZpfNWFOiVmwpXtsPAKVW9\nCFwUkc+BnkCFgW6Cw02tbmLdI+v43bLf8dzq53gn6x3qRtd1Arkw9zuz9bwRKZFXwrZudN0rX8fH\nxNMsvhnxMfHUjfru62W1Leu1utF1iYrwxV9ZY/yvZf2WjO8xnvE9xgPObcSXx9/99bmOL3roXYGX\ngJFADPAV8KCqbqnonNZDDz4rD6zkH+v/QVREVLkhezlYy3stJjKmVtwSaUygVKuHLiJvArcATUXk\nMPAMzpg5qvqKqm4TkY+BTUAJ8E9PYW6C0+D2gxncfnCgyzDGVJHHQFfV8V60+QvwF59UZIwxpkrs\ndgBjjAkTFujGGBMmLNCNMSZMWKAbY0yYsEA3xpgwYYFujDFhwgLdGGPCRMA2uBCRbKCqq3M1BU76\nsBx/C6V6Q6lWCK16Q6lWCK16Q6lWqF697VU1saw3Ahbo1SEiGeVNfQ1GoVRvKNUKoVVvKNUKoVVv\nKNUK/qvXhlyMMSZMWKAbY0yYCNVAnxboAioplOoNpVohtOoNpVohtOoNpVrBT/WG5Bi6McaY7wvV\nHroxxphrWKAbY0yYCLlAF5FRIrJDRHaLyL8Fup6KiMhrInJCRIJ+ww8RaSsiy0UkS0S2isjkQNdU\nHhGpIyJfiUhmaa1/CHRN3hCRSBHZICILA11LRURkv4hsFpGNIhL024qJSEMReVdEtovINhHpH+ia\nyiIiXUr/TC8/zonIEz69RiiNoYtIJM5epcNx9jL9GhivqlkBLawcIjIEuAC8Udb2fcFERFoCLVV1\nvYjUB9YBdwbjn604e9zFq+oFEYkGVgGTVXVNgEurkIg8ibOhegNVvT3Q9ZRHRPYDKaoaEhN1RGQG\nsFJV/ykiMUBdVT0T6LoqUpplR4CbVbWqEyy/J9R66H2B3aq6V1ULgLeAOwJcU7lU9XPgdKDr8Iaq\nHlPV9aVfnwe2Aa0DW1XZ1HGh9Nvo0kdQ90xEpA0wFvhnoGsJJyKSAAwBpgOoakGwh3mpW4E9vgxz\nCL1Abw0cuur7wwRp6ISy0k3BewNrA1tJ+UqHLzYCJ4BPVDVoay31AvB/cfbdDXYKLBGRdSLySKCL\n8aAjkA38b+lw1j9FJD7QRXnhQeBNX5801ALd+JmI1APmAk+o6rlA11MeVS1W1V5AG6CviATtkJaI\n3A6cUNV1ga7FS4NUtQ8wGni8dOgwWEUBfYC/q2pv4CIQ7J+txQDjgHd8fe5QC/QjQNurvm9T+prx\ngdLx6LnAbFWdF+h6vFH6z+vlwKhA11KBgcC40rHpt4BhIjIrsCWVT1WPlD6fAN7DGeoMVoeBw1f9\nC+1dnIAPZqOB9ap63NcnDrVA/xroLCIdS3/LPQgsCHBNYaH0g8bpwDZVfT7Q9VRERBJFpGHp13E4\nH5JvD2xV5VPV/6eqbVS1A87f2WWqmhbgssokIvGlH4pTOnQxAgjau7RU9RvgkIh0KX3pViDoPsi/\nxnj8MNwCzj9XQoaqFonIz4DFQCTwmqpuDXBZ5RKRN4FbgKYichh4RlWnB7aqcg0E3MDm0rFpgF+r\n6qIA1lSelsCM0jsFIoC3VTWobwUMIc2B95zf70QBc1T148CW5NHPgdmlnby9wMMBrqdcpb8khwOP\n+uX8oXTbojHGmPKF2pCLMcaYcligG2NMmLBAN8aYMGGBbowxYcIC3RhjwoQFujHGhAkLdGOMCRP/\nH21eLb9uiucWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2eiOeAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}